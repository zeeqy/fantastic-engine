{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from util.util import mnist_noise\n",
    "\n",
    "from trajectoryPlugin.plugin import API\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "def read_idx(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n",
    "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
    "        return np.frombuffer(f.read(), dtype=np.uint8).reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CNN\n",
    "\"\"\"\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.drop_out = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(7 * 7 * 64, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "def accuracy(predict_y, test_y):\n",
    "    score = 0\n",
    "    for pred, acc in zip(predict_y, test_y):\n",
    "        if pred == acc:\n",
    "            score +=1\n",
    "    return score / test_y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MNIST DATA\n",
    "\"\"\"\n",
    "x_train = read_idx('data/train-images.idx3-ubyte')\n",
    "y_train = read_idx('data/train-labels.idx1-ubyte')\n",
    "x_test = read_idx('data/t10k-images.idx3-ubyte')\n",
    "y_test = read_idx('data/t10k-labels.idx1-ubyte')\n",
    "valid_idx = np.random.choice(range(60000), size=1000, replace=False)\n",
    "x_valid = x_train[valid_idx]\n",
    "y_valid = y_train[valid_idx]\n",
    "x_train = np.delete(x_train, valid_idx, axis=0)\n",
    "y_train = np.delete(y_train, valid_idx)\n",
    "\n",
    "\"\"\"\n",
    "shrink dataset to make noisy significant\n",
    "\"\"\"\n",
    "subset_idx = np.random.choice(range(60000-1000), size=10000, replace=False)\n",
    "x_train = x_train[subset_idx]\n",
    "y_train = y_train[subset_idx]\n",
    "\n",
    "\"\"\"\n",
    "Add Noise label to training data\n",
    "\"\"\"\n",
    "y_train_noisy, noise_index = mnist_noise(y_train,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initializaion\n",
    "\"\"\"\n",
    "train_idx = np.arange(len(x_train))\n",
    "x_train = np.transpose(x_train,(2,1,0))\n",
    "x_valid = np.transpose(x_valid,(2,1,0))\n",
    "x_test = np.transpose(x_test,(2,1,0))\n",
    "x_train_tensor = torchvision.transforms.ToTensor()(x_train).unsqueeze(1)\n",
    "x_valid_tensor = torchvision.transforms.ToTensor()(x_valid).unsqueeze(1)\n",
    "x_test_tensor = torchvision.transforms.ToTensor()(x_test).unsqueeze(1)\n",
    "y_train_tensor = torch.from_numpy(y_train.astype(np.long))\n",
    "y_train_noisy_tensor = torch.from_numpy(y_train_noisy.astype(np.long))\n",
    "y_valid_tensor = torch.from_numpy(y_valid.astype(np.long))\n",
    "y_test_tensor = torch.from_numpy(y_test.astype(np.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================epoch = 1====================\n",
      "====================epoch = 2====================\n",
      "====================epoch = 3====================\n",
      "====================epoch = 4====================\n",
      "====================epoch = 5====================\n",
      "====================epoch = 6====================\n",
      "| - {0: 0, 'size': 2668, 'sim': '0.9808', 'num_special': 0, 'spe_ratio': '0.0000'}\n",
      "| - {1: 1, 'size': 1851, 'sim': '0.9839', 'num_special': 0, 'spe_ratio': '0.0000'}\n",
      "| - {2: 2, 'size': 498, 'sim': '-0.7839', 'num_special': 498, 'spe_ratio': '1.0000'}\n",
      "| - {3: 3, 'size': 3573, 'sim': '-0.7176', 'num_special': 0, 'spe_ratio': '0.0000'}\n",
      "| - {4: 4, 'size': 973, 'sim': '-0.5965', 'num_special': 70, 'spe_ratio': '0.0719'}\n",
      "| - {5: 5, 'size': 437, 'sim': '-0.7535', 'num_special': 432, 'spe_ratio': '0.9886'}\n",
      "====================epoch = 7====================\n",
      "| - {0: 0, 'size': 2655, 'sim': '0.9789', 'num_special': 0, 'spe_ratio': '0.0000'}\n",
      "| - {1: 1, 'size': 1132, 'sim': '0.9568', 'num_special': 38, 'spe_ratio': '0.0336'}\n",
      "| - {2: 2, 'size': 3464, 'sim': '0.9739', 'num_special': 0, 'spe_ratio': '0.0000'}\n",
      "| - {3: 3, 'size': 401, 'sim': '-0.7290', 'num_special': 383, 'spe_ratio': '0.9551'}\n",
      "| - {4: 4, 'size': 579, 'sim': '-0.9041', 'num_special': 579, 'spe_ratio': '1.0000'}\n",
      "| - {5: 5, 'size': 1769, 'sim': '-0.9012', 'num_special': 0, 'spe_ratio': '0.0000'}\n",
      "====================epoch = 8====================\n",
      "| - {0: 0, 'size': 406, 'sim': '-0.8919', 'num_special': 391, 'spe_ratio': '0.9631'}\n",
      "| - {1: 1, 'size': 1116, 'sim': '-0.8039', 'num_special': 42, 'spe_ratio': '0.0376'}\n",
      "| - {2: 2, 'size': 3450, 'sim': '-0.7459', 'num_special': 0, 'spe_ratio': '0.0000'}\n",
      "| - {3: 3, 'size': 567, 'sim': '-0.8792', 'num_special': 567, 'spe_ratio': '1.0000'}\n",
      "| - {4: 4, 'size': 2688, 'sim': '-0.8577', 'num_special': 0, 'spe_ratio': '0.0000'}\n",
      "| - {5: 5, 'size': 1773, 'sim': '-0.8508', 'num_special': 0, 'spe_ratio': '0.0000'}\n",
      "====================epoch = 9====================\n",
      "| - {0: 0, 'size': 827, 'sim': '-0.8809', 'num_special': 827, 'spe_ratio': '1.0000'}\n",
      "| - {1: 1, 'size': 1938, 'sim': '-0.8598', 'num_special': 0, 'spe_ratio': '0.0000'}\n",
      "| - {2: 2, 'size': 638, 'sim': '-0.8139', 'num_special': 172, 'spe_ratio': '0.2696'}\n",
      "| - {3: 3, 'size': 2229, 'sim': '-0.7619', 'num_special': 1, 'spe_ratio': '0.0004'}\n",
      "| - {4: 4, 'size': 1995, 'sim': '-0.7345', 'num_special': 0, 'spe_ratio': '0.0000'}\n",
      "| - {5: 5, 'size': 2373, 'sim': '-0.6507', 'num_special': 0, 'spe_ratio': '0.0000'}\n",
      "====================epoch = 10====================\n",
      "| - {0: 0, 'size': 2220, 'sim': '0.9694', 'num_special': 0, 'spe_ratio': '0.0000'}\n",
      "| - {1: 1, 'size': 1800, 'sim': '0.9761', 'num_special': 0, 'spe_ratio': '0.0000'}\n",
      "| - {2: 2, 'size': 2639, 'sim': '0.9803', 'num_special': 0, 'spe_ratio': '0.0000'}\n",
      "| - {3: 3, 'size': 1798, 'sim': '0.9804', 'num_special': 0, 'spe_ratio': '0.0000'}\n",
      "| - {4: 4, 'size': 699, 'sim': '0.9359', 'num_special': 156, 'spe_ratio': '0.2232'}\n",
      "| - {5: 5, 'size': 844, 'sim': '-0.8020', 'num_special': 844, 'spe_ratio': '1.0000'}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Here is an example of standard NN training + trajectory reweighting.\n",
    "\"\"\"\n",
    "\n",
    "# suppose there are training set and validation set, trajectory API initializaiton\n",
    "### currently, our API will take care of data part in training, see below\n",
    "api = API(x_train_tensor, \n",
    "          y_train_noisy_tensor, \n",
    "          x_valid_tensor, \n",
    "          y_valid_tensor, \n",
    "          num_cluster=6, \n",
    "          batch_size=100, \n",
    "          device=device, \n",
    "          iprint=2)\n",
    "\n",
    "# model and its paramters\n",
    "cnn = CNN()\n",
    "cnn.to(device)\n",
    "L2 = 0.0005\n",
    "learning_rate = 0.001\n",
    "num_iter = 10\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate, weight_decay=L2)\n",
    "\n",
    "# standard training starts\n",
    "epoch = 1\n",
    "while epoch <= num_iter:\n",
    "    print(\"=\"*20 + \"epoch = {}\".format(epoch) + \"=\"*20)\n",
    "    for step, (data, target, weight) in enumerate(api.train_loader):\n",
    "        data, target, weight = data.to(device), target.to(device), weight.to(device)\n",
    "        output = cnn(data)\n",
    "        loss = api.loss_func(output, target, weight)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # record trajectory\n",
    "    api.createTrajectory(cnn)\n",
    "    \n",
    "    # cluster trajectory + reweight data\n",
    "    if epoch > 5:\n",
    "        api.clusterTrajectory()\n",
    "        api.reweightData(cnn, optimizer, 2000, noise_index)\n",
    "        \n",
    "    epoch += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
