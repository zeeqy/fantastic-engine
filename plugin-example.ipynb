{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from util.util import mnist_noise\n",
    "from copy import deepcopy\n",
    "from scipy import spatial\n",
    "import torch.cuda as cutorch\n",
    "\n",
    "from trajectoryPlugin.plugin import API\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "def read_idx(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n",
    "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
    "        return np.frombuffer(f.read(), dtype=np.uint8).reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CNN\n",
    "\"\"\"\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        #self.drop_out = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(7 * 7 * 64, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        #out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "def accuracy(predict_y, test_y):\n",
    "    score = 0\n",
    "    for pred, acc in zip(predict_y, test_y):\n",
    "        if pred == acc:\n",
    "            score +=1\n",
    "    return score / test_y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MNIST DATA\n",
    "\"\"\"\n",
    "x_train = read_idx('data/train-images.idx3-ubyte')\n",
    "y_train = read_idx('data/train-labels.idx1-ubyte')\n",
    "x_test = read_idx('data/t10k-images.idx3-ubyte')\n",
    "y_test = read_idx('data/t10k-labels.idx1-ubyte')\n",
    "valid_idx = np.random.choice(range(60000), size=1000, replace=False)\n",
    "x_valid = x_train[valid_idx]\n",
    "y_valid = y_train[valid_idx]\n",
    "x_train = np.delete(x_train, valid_idx, axis=0)\n",
    "y_train = np.delete(y_train, valid_idx)\n",
    "\n",
    "\"\"\"\n",
    "shrink dataset to make noisy significant\n",
    "\"\"\"\n",
    "subset_idx = np.random.choice(range(60000-1000), size=10000, replace=False)\n",
    "x_train = x_train[subset_idx]\n",
    "y_train = y_train[subset_idx]\n",
    "\n",
    "\"\"\"\n",
    "Add Noise label to training data\n",
    "\"\"\"\n",
    "y_train_noisy, noise_index = mnist_noise(y_train,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initializaion\n",
    "\"\"\"\n",
    "train_idx = np.arange(len(x_train))\n",
    "x_train = np.transpose(x_train,(2,1,0))\n",
    "x_valid = np.transpose(x_valid,(2,1,0))\n",
    "x_test = np.transpose(x_test,(2,1,0))\n",
    "x_train_tensor = torchvision.transforms.ToTensor()(x_train).unsqueeze(1)\n",
    "x_valid_tensor = torchvision.transforms.ToTensor()(x_valid).unsqueeze(1)\n",
    "x_test_tensor = torchvision.transforms.ToTensor()(x_test).unsqueeze(1)\n",
    "y_train_tensor = torch.from_numpy(y_train.astype(np.long))\n",
    "y_train_noisy_tensor = torch.from_numpy(y_train_noisy.astype(np.long))\n",
    "y_valid_tensor = torch.from_numpy(y_valid.astype(np.long))\n",
    "y_test_tensor = torch.from_numpy(y_test.astype(np.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================epoch = 1====================\n",
      "Memory  51669504   161115136   6370295808\n",
      "====================epoch = 2====================\n",
      "Memory  51669504   161115136   6370295808\n",
      "====================epoch = 3====================\n",
      "Memory  51669504   161115136   6370295808\n",
      "====================epoch = 4====================\n",
      "Memory  51669504   161115136   6370295808\n",
      "====================epoch = 5====================\n",
      "Memory  51669504   161115136   6370295808\n",
      "====================epoch = 6====================\n",
      "| - {0: 0, 'size': 947, 'sim': '-0.9255', 'num_special': 937, 'spe_ratio': '0.9894'}\n",
      "| - {1: 1, 'size': 3212, 'sim': '0.9736', 'num_special': 63, 'spe_ratio': '0.0196'}\n",
      "| - {2: 2, 'size': 5841, 'sim': '0.9885', 'num_special': 0, 'spe_ratio': '0.0000'}\n",
      "Memory  51669504   986818560   6370295808\n",
      "====================epoch = 7====================\n",
      "| - {0: 0, 'size': 937, 'sim': '-0.7542', 'num_special': 927, 'spe_ratio': '0.9893'}\n",
      "| - {1: 1, 'size': 3215, 'sim': '0.9465', 'num_special': 73, 'spe_ratio': '0.0227'}\n",
      "| - {2: 2, 'size': 5848, 'sim': '0.9725', 'num_special': 0, 'spe_ratio': '0.0000'}\n",
      "Memory  51669504   986818560   6370295808\n",
      "====================epoch = 8====================\n",
      "| - {0: 0, 'size': 5819, 'sim': '0.9705', 'num_special': 0, 'spe_ratio': '0.0000'}\n",
      "| - {1: 1, 'size': 3222, 'sim': '0.9791', 'num_special': 52, 'spe_ratio': '0.0161'}\n",
      "| - {2: 2, 'size': 959, 'sim': '-0.9020', 'num_special': 948, 'spe_ratio': '0.9885'}\n",
      "Memory  51669504   986818560   6370295808\n",
      "====================epoch = 9====================\n",
      "| - {0: 0, 'size': 5818, 'sim': '0.9787', 'num_special': 0, 'spe_ratio': '0.0000'}\n",
      "| - {1: 1, 'size': 3220, 'sim': '0.9555', 'num_special': 49, 'spe_ratio': '0.0152'}\n",
      "| - {2: 2, 'size': 962, 'sim': '-0.8577', 'num_special': 951, 'spe_ratio': '0.9886'}\n",
      "Memory  51669504   986818560   6370295808\n",
      "====================epoch = 10====================\n",
      "| - {0: 0, 'size': 971, 'sim': '-0.8303', 'num_special': 959, 'spe_ratio': '0.9876'}\n",
      "| - {1: 1, 'size': 5692, 'sim': '0.9502', 'num_special': 0, 'spe_ratio': '0.0000'}\n",
      "| - {2: 2, 'size': 3337, 'sim': '0.9575', 'num_special': 41, 'spe_ratio': '0.0123'}\n",
      "Memory  51669504   986818560   6370295808\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Here is an example of standard NN training + trajectory reweighting.\n",
    "\"\"\"\n",
    "\n",
    "# suppose there are training set and validation set, trajectory API initializaiton\n",
    "### currently, our API will take care of data part in training, see below\n",
    "api = API(num_cluster=3, device=device, iprint=2)\n",
    "\n",
    "# first we use data tensor for initializaiton\n",
    "api.dataTensor(x_train_tensor, y_train_noisy_tensor, x_valid_tensor, y_valid_tensor, batch_size=100)\n",
    "\n",
    "# model and its paramters\n",
    "cnn = CNN()\n",
    "cnn.to(device)\n",
    "L2 = 0.0005\n",
    "learning_rate = 0.001\n",
    "num_iter = 10\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate, weight_decay=L2)\n",
    "\n",
    "# standard training starts\n",
    "epoch = 1\n",
    "while epoch <= num_iter:\n",
    "    print(\"=\"*20 + \"epoch = {}\".format(epoch) + \"=\"*20)\n",
    "    for step, (data, target, weight) in enumerate(api.train_loader): # api train_loader\n",
    "        data, target, weight = data.to(device), target.to(device), weight.to(device)\n",
    "        output = cnn(data)\n",
    "        loss = api.loss_func(output, target, weight) # api train_loader\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # record trajectory\n",
    "    api.createTrajectory(cnn)\n",
    "    \n",
    "    # cluster trajectory + reweight data\n",
    "    if epoch > 5:\n",
    "        api.clusterTrajectory() # run gmm cluster\n",
    "        api.reweightData(cnn, 2000, noise_index) # update train_loader\n",
    "    \n",
    "    print(\"Memory \",str(cutorch.memory_allocated(0)) , ' ', str(cutorch.max_memory_allocated(0)) , ' ' , str(cutorch.get_device_properties(0).total_memory))\n",
    "    \n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "train_dataset = Data.TensorDataset(x_train_tensor, y_train_noisy_tensor)\n",
    "train_loader = Data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_dataset = Data.TensorDataset(x_valid_tensor, y_valid_tensor)\n",
    "\n",
    "# or we can use data loader for initializaiton\n",
    "### suppose data are form into loader first\n",
    "api.dataLoader(train_loader, valid_loader)\n",
    "\n",
    "# model and its paramters\n",
    "cnn = CNN()\n",
    "cnn.to(device)\n",
    "L2 = 0.0005\n",
    "learning_rate = 0.001\n",
    "num_iter = 10\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate, weight_decay=L2)\n",
    "\n",
    "# standard training starts\n",
    "epoch = 1\n",
    "while epoch <= num_iter:\n",
    "    print(\"=\"*20 + \"epoch = {}\".format(epoch) + \"=\"*20)\n",
    "    for step, (data, target, weight) in enumerate(api.train_loader): # api train_loader\n",
    "        data, target, weight = data.to(device), target.to(device), weight.to(device)\n",
    "        output = cnn(data)\n",
    "        loss = api.loss_func(output, target, weight) # api train_loader\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # record trajectory\n",
    "    api.createTrajectory(cnn)\n",
    "    \n",
    "    # cluster trajectory + reweight data\n",
    "    if epoch > 5:\n",
    "        api.clusterTrajectory() # run gmm cluster\n",
    "        api.reweightData(cnn, 2000, noise_index) # update train_loader\n",
    "        \n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Memory \",str(cutorch.memory_allocated(0)) , ' ', str(cutorch.max_memory_allocated(0)) , ' ' , str(cutorch.get_device_properties(0).total_memory))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
