{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from util.util import mnist_noise\n",
    "from copy import deepcopy\n",
    "from scipy import spatial\n",
    "import torch.cuda as cutorch\n",
    "\n",
    "from trajectoryPlugin.plugin import API\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "def read_idx(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n",
    "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
    "        return np.frombuffer(f.read(), dtype=np.uint8).reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CNN\n",
    "\"\"\"\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.drop_out = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(7 * 7 * 64, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "def accuracy(predict_y, test_y):\n",
    "    score = 0\n",
    "    for pred, acc in zip(predict_y, test_y):\n",
    "        if pred == acc:\n",
    "            score +=1\n",
    "    return score / test_y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MNIST DATA\n",
    "\"\"\"\n",
    "batch_size = 100\n",
    "\n",
    "mnistdata = datasets.MNIST('../data', train=True, download=True,\n",
    "             transform=transforms.Compose([\n",
    "                 transforms.ToTensor(),\n",
    "                 transforms.Normalize((0.1307,), (0.3081,))\n",
    "             ]))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "datalen = len(mnistdata)\n",
    "valid_index = np.random.choice(range(len(mnistdata)), size=5000, replace=False).tolist()\n",
    "train_index = np.delete(range(len(mnistdata)), valid_index).tolist()\n",
    "trainset = torch.utils.data.dataset.Subset(mnistdata, train_index)\n",
    "validset = torch.utils.data.dataset.Subset(mnistdata, valid_index)\n",
    "\n",
    "\"\"\"\n",
    "Add Noise label to training data\n",
    "\"\"\"\n",
    "noise_idx = []\n",
    "noise_idx = np.random.choice(range(len(trainset)), size=int(len(trainset)* 0.1), replace=False)\n",
    "label = range(10)\n",
    "for idx in noise_idx:\n",
    "    true_label = trainset.dataset.targets[train_index[idx]]\n",
    "    noise_label = [lab for lab in label if lab != true_label]\n",
    "    trainset.dataset.targets[train_index[idx]] = int(np.random.choice(noise_label))\n",
    "    \n",
    "\n",
    "# suppose there are training set and validation set, trajectory API initializaiton\n",
    "### currently, our API will take care of data part in training, see below\n",
    "\n",
    "# we use torch dataset for initializaiton\n",
    "api = API(num_cluster=3, device=device, iprint=2)\n",
    "api.dataLoader(trainset, validset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================epoch = 1====================\n",
      "Test Loss = 0.3496535747528076, Test Accuracy = 94.61\n",
      "Memory  39697920   149143040   6370295808\n",
      "====================epoch = 2====================\n",
      "Test Loss = 0.27012772369384763, Test Accuracy = 96.42\n",
      "Memory  39697920   149144064   6370295808\n",
      "====================epoch = 3====================\n",
      "Test Loss = 0.2503749544143677, Test Accuracy = 97.24\n",
      "Memory  39697920   149144064   6370295808\n",
      "====================epoch = 4====================\n",
      "2019-04-05 01:35:12,159 - INFO - | - {0: 0, 'size': 10746, 'sim': '-0.9305', 'num_special': 5494, 'spe_ratio': '0.5113'}\n",
      "2019-04-05 01:35:12,164 - INFO - | - {1: 1, 'size': 18202, 'sim': '0.9882', 'num_special': 5, 'spe_ratio': '0.0003'}\n",
      "2019-04-05 01:35:12,172 - INFO - | - {2: 2, 'size': 26052, 'sim': '0.9413', 'num_special': 1, 'spe_ratio': '0.0000'}\n",
      "Test Loss = 0.2223454174041748, Test Accuracy = 97.59\n",
      "Memory  39697920   149463552   6370295808\n",
      "====================epoch = 5====================\n",
      "2019-04-05 01:35:38,405 - INFO - | - {0: 0, 'size': 26159, 'sim': '0.9541', 'num_special': 1, 'spe_ratio': '0.0000'}\n",
      "2019-04-05 01:35:38,410 - INFO - | - {1: 1, 'size': 17997, 'sim': '0.9894', 'num_special': 4, 'spe_ratio': '0.0002'}\n",
      "2019-04-05 01:35:38,415 - INFO - | - {2: 2, 'size': 10844, 'sim': '-0.8881', 'num_special': 5495, 'spe_ratio': '0.5067'}\n",
      "Test Loss = 0.19435113639831542, Test Accuracy = 97.86\n",
      "Memory  39697920   149463552   6370295808\n",
      "====================epoch = 6====================\n",
      "2019-04-05 01:36:02,996 - INFO - | - {0: 0, 'size': 10934, 'sim': '-0.8708', 'num_special': 5495, 'spe_ratio': '0.5026'}\n",
      "2019-04-05 01:36:03,003 - INFO - | - {1: 1, 'size': 26079, 'sim': '0.9500', 'num_special': 0, 'spe_ratio': '0.0000'}\n",
      "2019-04-05 01:36:03,009 - INFO - | - {2: 2, 'size': 17987, 'sim': '0.9875', 'num_special': 5, 'spe_ratio': '0.0003'}\n",
      "Test Loss = 0.17186555271148682, Test Accuracy = 98.04\n",
      "Memory  39697920   149463552   6370295808\n",
      "====================epoch = 7====================\n",
      "2019-04-05 01:36:28,117 - INFO - | - {0: 0, 'size': 17810, 'sim': '0.9865', 'num_special': 4, 'spe_ratio': '0.0002'}\n",
      "2019-04-05 01:36:28,122 - INFO - | - {1: 1, 'size': 11155, 'sim': '-0.9653', 'num_special': 5496, 'spe_ratio': '0.4927'}\n",
      "2019-04-05 01:36:28,132 - INFO - | - {2: 2, 'size': 26035, 'sim': '0.9281', 'num_special': 0, 'spe_ratio': '0.0000'}\n",
      "Test Loss = 0.1469354298591614, Test Accuracy = 98.19\n",
      "Memory  39697920   149463552   6370295808\n",
      "====================epoch = 8====================\n",
      "2019-04-05 01:36:53,010 - INFO - | - {0: 0, 'size': 25998, 'sim': '0.9124', 'num_special': 0, 'spe_ratio': '0.0000'}\n",
      "2019-04-05 01:36:53,016 - INFO - | - {1: 1, 'size': 11278, 'sim': '-0.8673', 'num_special': 5497, 'spe_ratio': '0.4874'}\n",
      "2019-04-05 01:36:53,021 - INFO - | - {2: 2, 'size': 17724, 'sim': '0.9827', 'num_special': 3, 'spe_ratio': '0.0002'}\n",
      "Test Loss = 0.1311823106765747, Test Accuracy = 98.37\n",
      "Memory  39697920   149463552   6370295808\n",
      "====================epoch = 9====================\n",
      "2019-04-05 01:37:18,582 - INFO - | - {0: 0, 'size': 11331, 'sim': '-0.9040', 'num_special': 5497, 'spe_ratio': '0.4851'}\n",
      "2019-04-05 01:37:18,589 - INFO - | - {1: 1, 'size': 26079, 'sim': '0.8968', 'num_special': 0, 'spe_ratio': '0.0000'}\n",
      "2019-04-05 01:37:18,595 - INFO - | - {2: 2, 'size': 17590, 'sim': '0.9709', 'num_special': 3, 'spe_ratio': '0.0002'}\n",
      "Test Loss = 0.11229171872138977, Test Accuracy = 98.28\n",
      "Memory  39697920   149463552   6370295808\n",
      "====================epoch = 10====================\n",
      "2019-04-05 01:37:43,499 - INFO - | - {0: 0, 'size': 17434, 'sim': '0.9440', 'num_special': 3, 'spe_ratio': '0.0002'}\n",
      "2019-04-05 01:37:43,506 - INFO - | - {1: 1, 'size': 26048, 'sim': '0.8552', 'num_special': 0, 'spe_ratio': '0.0000'}\n",
      "2019-04-05 01:37:43,511 - INFO - | - {2: 2, 'size': 11518, 'sim': '-0.9153', 'num_special': 5497, 'spe_ratio': '0.4773'}\n",
      "Test Loss = 0.09410108804702759, Test Accuracy = 98.39\n",
      "Memory  39697920   149463552   6370295808\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Here is an example of standard NN training + trajectory reweighting.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# model and its paramters\n",
    "cnn = CNN()\n",
    "cnn.to(device)\n",
    "L2 = 0.0005\n",
    "learning_rate = 0.001\n",
    "num_iter = 10\n",
    "#optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate, weight_decay=L2)\n",
    "optimizer = torch.optim.SGD(cnn.parameters(), lr=learning_rate, momentum=0.9)\n",
    "# standard training starts\n",
    "epoch = 1\n",
    "while epoch <= num_iter:\n",
    "    print(\"=\"*20 + \"epoch = {}\".format(epoch) + \"=\"*20)\n",
    "    cnn.train()\n",
    "    for step, (data, target, idx) in enumerate(api.train_loader): # api train_loader\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        weight = api.weight_tensor[idx].to(device)\n",
    "        output = cnn(data)\n",
    "        loss = api.loss_func(output, target, weight) # api train_loader\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # record trajectory\n",
    "    api.createTrajectory(cnn)\n",
    "    \n",
    "    # cluster trajectory + reweight data\n",
    "    if epoch > 3:\n",
    "        api.clusterTrajectory() # run gmm cluster\n",
    "        api.reweightData(cnn, 1e6, noise_idx) # update train_loader\n",
    "        \n",
    "    cnn.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = cnn(data)\n",
    "            loss += api.loss_func(output, target, None, 'sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    \n",
    "    print('Test Loss = {}, Test Accuracy = {}'.format(loss,accuracy))\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Memory \",str(cutorch.memory_allocated(0)) , ' ', str(cutorch.max_memory_allocated(0)) , ' ' , str(cutorch.get_device_properties(0).total_memory))\n",
    "    \n",
    "    epoch += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
