{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "from copy import deepcopy\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from scipy import spatial\n",
    "from util.util import mnist_noise, correct_prob\n",
    "from util.util import ImbalancedDatasetSampler\n",
    "\n",
    "from trajectoryReweight.model import WeightedCrossEntropyLoss, TrajectoryReweightNN\n",
    "from trajectoryReweight.baseline import StandardTrainingNN\n",
    "from trajectoryReweight.gmm import GaussianMixture\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "def read_idx(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n",
    "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
    "        return np.frombuffer(f.read(), dtype=np.uint8).reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CNN & LeNet\n",
    "\"\"\"\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.drop_out = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(7 * 7 * 64, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def name(self):\n",
    "        return \"LeNet\"\n",
    "\n",
    "def accuracy(predict_y, test_y):\n",
    "    score = 0\n",
    "    for pred, acc in zip(predict_y, test_y):\n",
    "        if pred == acc:\n",
    "            score +=1\n",
    "    return score / test_y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "MNIST DATA\n",
    "\"\"\"\n",
    "x_train = read_idx('data/train-images.idx3-ubyte')\n",
    "y_train = read_idx('data/train-labels.idx1-ubyte')\n",
    "x_test = read_idx('data/t10k-images.idx3-ubyte')\n",
    "y_test = read_idx('data/t10k-labels.idx1-ubyte')\n",
    "valid_idx = np.random.choice(range(60000), size=1000, replace=False)\n",
    "x_valid = x_train[valid_idx]\n",
    "y_valid = y_train[valid_idx]\n",
    "x_train = np.delete(x_train, valid_idx, axis=0)\n",
    "y_train = np.delete(y_train, valid_idx)\n",
    "\n",
    "\"\"\"\n",
    "shrink dataset to make noisy significant\n",
    "\"\"\"\n",
    "# x_train = x_train[:10000]\n",
    "# y_train = y_train[:10000]\n",
    "\n",
    "\"\"\"\n",
    "Add Noise to training data\n",
    "\"\"\"\n",
    "y_train_noisy = mnist_noise(y_train,0.8)\n",
    "\n",
    "\"\"\"\n",
    "Plot an example\n",
    "\"\"\" \n",
    "plt.imshow(x_train[0], cmap='gray')\n",
    "plt.title('%i' % y_train[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initializaion\n",
    "\"\"\"\n",
    "train_idx = np.arange(len(x_train))\n",
    "x_train = np.transpose(x_train,(2,1,0))\n",
    "x_valid = np.transpose(x_valid,(2,1,0))\n",
    "x_test = np.transpose(x_test,(2,1,0))\n",
    "x_train_tensor = torchvision.transforms.ToTensor()(x_train).unsqueeze(1)\n",
    "x_valid_tensor = torchvision.transforms.ToTensor()(x_valid).unsqueeze(1)\n",
    "x_test_tensor = torchvision.transforms.ToTensor()(x_test).unsqueeze(1)\n",
    "y_train_tensor = torch.from_numpy(y_train.astype(np.long))\n",
    "y_train_noisy_tensor = torch.from_numpy(y_train_noisy.astype(np.long))\n",
    "y_valid_tensor = torch.from_numpy(y_valid.astype(np.long))\n",
    "y_test_tensor = torch.from_numpy(y_test.astype(np.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CNN without reweight\n",
    "\"\"\"\n",
    "cnn = CNN()\n",
    "cnn.to(device)\n",
    "\n",
    "stand_trainNN = StandardTrainingNN(cnn,\n",
    "                                   batch_size=64,\n",
    "                                   num_iter=80,\n",
    "                                   learning_rate=1e-3,\n",
    "                                   early_stopping=5,\n",
    "                                   iprint=1)\n",
    "stand_trainNN.fit(x_train_tensor, y_train_noisy_tensor, x_valid_tensor, y_valid_tensor, x_test_tensor, y_test_tensor, device)\n",
    "\n",
    "test_output_y = stand_trainNN.predict(x_test_tensor,device)\n",
    "test_accuracy = accuracy(test_output_y, y_test_tensor.data.numpy())\n",
    "\n",
    "print('test accuracy is {}%'.format(100 * test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 5 burn-in epoch...\n",
      "Train 5 burn-in epoch complete.\n",
      "------------------------------------------------------------\n",
      "Trajectory clustering for burn-in epoch...\n",
      "Trajectory clustering for burn-in epoch complete.\n",
      "------------------------------------------------------------\n",
      "Trajectory based training start ...\n",
      "\n",
      "epoch = 6 | training loss = 2.1518 | valid loss = 1.9676 | early stopping = 0/5 | test loss = 0.0308 | test accuarcy = 83.06% [8306/10000]\n",
      "epoch = 7 | training loss = 2.1494 | valid loss = 1.9927 | early stopping = 1/5 | test loss = 0.0312 | test accuarcy = 87.63% [8763/10000]\n",
      "epoch = 8 | training loss = 2.2574 | valid loss = 1.9487 | early stopping = 0/5 | test loss = 0.0306 | test accuarcy = 90.67% [9067/10000]\n",
      "epoch = 9 | training loss = 2.2550 | valid loss = 1.8971 | early stopping = 0/5 | test loss = 0.0298 | test accuarcy = 92.75% [9275/10000]\n",
      "epoch = 10 | training loss = 2.2539 | valid loss = 1.8919 | early stopping = 0/5 | test loss = 0.0297 | test accuarcy = 90.88% [9088/10000]\n",
      "epoch = 11 | training loss = 2.2435 | valid loss = 1.8713 | early stopping = 0/5 | test loss = 0.0293 | test accuarcy = 93.66% [9366/10000]\n",
      "epoch = 12 | training loss = 2.2426 | valid loss = 1.9165 | early stopping = 1/5 | test loss = 0.0300 | test accuarcy = 93.45% [9345/10000]\n",
      "epoch = 13 | training loss = 2.2420 | valid loss = 1.8742 | early stopping = 2/5 | test loss = 0.0294 | test accuarcy = 93.78% [9378/10000]\n",
      "epoch = 14 | training loss = 1.9234 | valid loss = 1.7286 | early stopping = 0/5 | test loss = 0.0271 | test accuarcy = 93.52% [9352/10000]\n",
      "epoch = 15 | training loss = 1.9202 | valid loss = 1.7254 | early stopping = 0/5 | test loss = 0.0270 | test accuarcy = 94.1% [9410/10000]\n",
      "epoch = 16 | training loss = 1.9198 | valid loss = 1.6957 | early stopping = 0/5 | test loss = 0.0264 | test accuarcy = 93.69% [9369/10000]\n",
      "epoch = 17 | training loss = 2.1356 | valid loss = 1.8107 | early stopping = 1/5 | test loss = 0.0284 | test accuarcy = 94.79% [9479/10000]\n",
      "epoch = 18 | training loss = 2.1351 | valid loss = 1.8035 | early stopping = 2/5 | test loss = 0.0283 | test accuarcy = 94.61% [9461/10000]\n",
      "epoch = 19 | training loss = 2.1340 | valid loss = 1.8180 | early stopping = 3/5 | test loss = 0.0284 | test accuarcy = 95.03% [9503/10000]\n",
      "epoch = 20 | training loss = 2.2598 | valid loss = 1.8609 | early stopping = 4/5 | test loss = 0.0292 | test accuarcy = 95.24% [9524/10000]\n",
      "epoch = 21 | training loss = 2.2594 | valid loss = 1.8097 | early stopping = 5/5 | test loss = 0.0284 | test accuarcy = 95.56% [9556/10000]\n",
      "Trajectory based training complete, best validation loss = 1.6957359462976456 at epoch = 16.\n",
      "test accuracy is 93.69%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CNN with reweight\n",
    "\"\"\"\n",
    "cnn = CNN()\n",
    "cnn.to(device)\n",
    "\n",
    "tra_weightNN = TrajectoryReweightNN(cnn,\n",
    "                                    burnin=5,\n",
    "                                    num_cluster=10,\n",
    "                                    batch_size=64,\n",
    "                                    num_iter=75,\n",
    "                                    learning_rate=1e-3,\n",
    "                                    early_stopping=5,\n",
    "                                    device=device,\n",
    "                                    iprint=1)\n",
    "tra_weightNN.fit(x_train_tensor, y_train_noisy_tensor, x_valid_tensor, y_valid_tensor,x_test_tensor, y_test_tensor)\n",
    "\n",
    "test_output_y = tra_weightNN.predict(x_test_tensor)\n",
    "test_accuracy = accuracy(test_output_y, y_test_tensor.data.numpy())\n",
    "\n",
    "print('test accuracy is {}%'.format(100 * test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Repeat 10 times\n",
    "\"\"\"\n",
    "\n",
    "std_test = []\n",
    "rewgt_test = []\n",
    "\n",
    "for rep in range(10):\n",
    "    \"\"\"\n",
    "    prepare data\n",
    "    \"\"\"\n",
    "    x_train = read_idx('data/train-images.idx3-ubyte')\n",
    "    y_train = read_idx('data/train-labels.idx1-ubyte')\n",
    "    x_test = read_idx('data/t10k-images.idx3-ubyte')\n",
    "    y_test = read_idx('data/t10k-labels.idx1-ubyte')\n",
    "    valid_idx = np.random.choice(range(60000), size=1000, replace=False)\n",
    "    x_valid = x_train[valid_idx]\n",
    "    y_valid = y_train[valid_idx]\n",
    "    x_train = np.delete(x_train, valid_idx, axis=0)\n",
    "    y_train = np.delete(y_train, valid_idx)\n",
    "\n",
    "    \"\"\"\n",
    "    shrink dataset to make noisy significant\n",
    "    \"\"\"\n",
    "#     x_train = x_train[:10000]\n",
    "#     y_train = y_train[:10000]\n",
    "\n",
    "    \"\"\"\n",
    "    Add Noise to training data\n",
    "    \"\"\"\n",
    "    y_train_noisy = mnist_noise(y_train,0.1)\n",
    "    \n",
    "    \"\"\"\n",
    "    to tensor\n",
    "    \"\"\"\n",
    "    train_idx = np.arange(len(x_train))\n",
    "    x_train = np.transpose(x_train,(2,1,0))\n",
    "    x_valid = np.transpose(x_valid,(2,1,0))\n",
    "    x_test = np.transpose(x_test,(2,1,0))\n",
    "    x_train_tensor = torchvision.transforms.ToTensor()(x_train).unsqueeze(1)\n",
    "    x_valid_tensor = torchvision.transforms.ToTensor()(x_valid).unsqueeze(1)\n",
    "    x_test_tensor = torchvision.transforms.ToTensor()(x_test).unsqueeze(1)\n",
    "    y_train_tensor = torch.from_numpy(y_train.astype(np.long))\n",
    "    y_train_noisy_tensor = torch.from_numpy(y_train_noisy.astype(np.long))\n",
    "    y_valid_tensor = torch.from_numpy(y_valid.astype(np.long))\n",
    "    y_test_tensor = torch.from_numpy(y_test.astype(np.long))\n",
    "    \n",
    "    \"\"\"\n",
    "    comparing\n",
    "    \"\"\"\n",
    "    cnn = CNN()\n",
    "    cnn.to(device)\n",
    "\n",
    "    stand_trainNN = StandardTrainingNN(cnn,\n",
    "                                       batch_size=64, \n",
    "                                       num_iter=80,\n",
    "                                       learning_rate=0.001,\n",
    "                                       early_stopping=5,\n",
    "                                       iprint=0)\n",
    "    stand_trainNN.fit(x_train_tensor, y_train_noisy_tensor, x_valid_tensor, y_valid_tensor,device)\n",
    "    test_output_y = stand_trainNN.predict(x_test_tensor,device)\n",
    "    test_accuracy = accuracy(test_output_y, y_test_tensor.data.numpy())\n",
    "    std_test.append(test_accuracy)\n",
    "    \n",
    "    time.sleep(10)\n",
    "    \n",
    "    cnn = CNN()\n",
    "    cnn.to(device)\n",
    "\n",
    "    tra_weightNN = TrajectoryReweightNN(cnn,\n",
    "                                        burnin=5, \n",
    "                                        num_cluster=10,\n",
    "                                        batch_size=64, \n",
    "                                        num_iter=75,\n",
    "                                        learning_rate=0.001,\n",
    "                                        early_stopping=5,\n",
    "                                        iprint=0)\n",
    "    tra_weightNN.fit(x_train_tensor, y_train_noisy_tensor, x_valid_tensor, y_valid_tensor,device)\n",
    "    test_output_y = tra_weightNN.predict(x_test_tensor,device)\n",
    "    test_accuracy = accuracy(test_output_y, y_test_tensor.data.numpy())\n",
    "    rewgt_test.append(test_accuracy)\n",
    "    \n",
    "    time.sleep(10)\n",
    "    \n",
    "    print('running {}/10'.format(rep+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(std_test)\n",
    "print(rewgt_test)\n",
    "print(np.mean(std_test),np.mean(rewgt_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imbalanced MNIST DATA\n",
    "\"\"\"\n",
    "x_train = read_idx('data/train-images.idx3-ubyte')\n",
    "y_train = read_idx('data/train-labels.idx1-ubyte')\n",
    "x_test = read_idx('data/t10k-images.idx3-ubyte')\n",
    "y_test = read_idx('data/t10k-labels.idx1-ubyte')\n",
    "\n",
    "four_index = y_train == 4\n",
    "nine_index = y_train == 9\n",
    "\n",
    "y_fours = y_train[four_index]-4\n",
    "y_nines = y_train[nine_index]-8\n",
    "x_fours = x_train[four_index]\n",
    "x_nines = x_train[nine_index]\n",
    "\n",
    "valid_four_index = np.random.choice(range(len(y_fours)), size=250, replace=False)\n",
    "valid_nine_index = np.random.choice(range(len(y_nines)), size=250, replace=False)\n",
    "\n",
    "x_valid_four =  x_fours[valid_four_index]\n",
    "x_valid_nine =  x_nines[valid_nine_index]\n",
    "y_valid_four =  y_fours[valid_four_index]\n",
    "y_valid_nine =  y_nines[valid_nine_index]\n",
    "\n",
    "x_valid = np.append(x_valid_four,x_valid_nine,0)\n",
    "y_valid = np.append(y_valid_four,y_valid_nine)\n",
    "indices = np.arange(x_valid.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "x_valid = x_valid[indices]\n",
    "y_valid = y_valid[indices]\n",
    "\n",
    "x_train_four = np.delete(x_fours, valid_four_index, axis=0)\n",
    "y_train_four = np.delete(y_fours, valid_four_index)\n",
    "x_train_nine = np.delete(x_nines, valid_four_index, axis=0)\n",
    "y_train_nine = np.delete(y_nines, valid_four_index)\n",
    "\n",
    "four_index = dataset['y_test'] == 4\n",
    "nine_index = dataset['y_test'] == 9\n",
    "\n",
    "y_fours = dataset['y_test'][four_index]-4\n",
    "y_nines = dataset['y_test'][nine_index]-8\n",
    "x_fours = dataset['x_test'][four_index]\n",
    "x_nines = dataset['x_test'][nine_index]\n",
    "x_test = np.append(x_fours,x_nines,0)\n",
    "y_test = np.append(y_fours,y_nines)\n",
    "indices = np.arange(x_test.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "x_test = x_test[indices]\n",
    "y_test = y_test[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imbalanced MNIST Study\n",
    "\"\"\" \n",
    "a = [500,250,150,50,25]\n",
    "\n",
    "ratio = 500\n",
    "print('ratio: 4:9 = {}:{}'.format(ratio, 5000-ratio))\n",
    "four_part = np.random.choice(range(len(x_train_four)), size=ratio, replace=False)\n",
    "nine_part = np.random.choice(range(len(x_train_nine)), size=5000-ratio, replace=False)\n",
    "x_train = np.append(x_train_four[four_part],x_train_nine[nine_part],0)\n",
    "y_train = np.append(y_train_four[four_part],y_train_nine[nine_part])\n",
    "indices = np.arange(x_train.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "x_train = x_train[indices]\n",
    "y_train = y_train[indices]\n",
    "\n",
    "\n",
    "x_train = np.transpose(x_train,(2,1,0))\n",
    "x_valid = np.transpose(x_valid,(2,1,0))\n",
    "x_test = np.transpose(x_test,(2,1,0))\n",
    "x_train_tensor = torchvision.transforms.ToTensor()(x_train).unsqueeze(1)\n",
    "x_valid_tensor = torchvision.transforms.ToTensor()(x_valid).unsqueeze(1)\n",
    "x_test_tensor = torchvision.transforms.ToTensor()(x_test).unsqueeze(1)\n",
    "y_train_tensor = torch.from_numpy(y_train.astype(np.long))\n",
    "y_valid_tensor = torch.from_numpy(y_valid.astype(np.long))\n",
    "y_test_tensor = torch.from_numpy(y_test.astype(np.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imbalanced MNIST without reweight\n",
    "\"\"\"\n",
    "lenet = LeNet()\n",
    "lenet.to(device)\n",
    "\n",
    "stand_trainNN = StandardTrainingNN(lenet,\n",
    "                                   batch_size=100, \n",
    "                                   num_iter=80,\n",
    "                                   learning_rate=1e-3,\n",
    "                                   early_stopping=10,\n",
    "                                   iprint=1)\n",
    "stand_trainNN.fit(x_train_tensor, y_train_tensor, x_valid_tensor, y_valid_tensor,device)\n",
    "\n",
    "test_output_y = stand_trainNN.predict(x_test_tensor,device)\n",
    "test_accuracy = accuracy(test_output_y, y_test_tensor.data.numpy())\n",
    "\n",
    "print('test accuracy is {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imbalanced MNIST with reweight\n",
    "\"\"\"\n",
    "lenet = LeNet()\n",
    "lenet.to(device)\n",
    "\n",
    "tra_weightNN = TrajectoryReweightNN(lenet,\n",
    "                                    burnin=10, \n",
    "                                    num_cluster=5,\n",
    "                                    batch_size=50, \n",
    "                                    num_iter=70, \n",
    "                                    learning_rate=1e-3,\n",
    "                                    early_stopping=10,\n",
    "                                    iprint=1)\n",
    "tra_weightNN.fit(x_train_tensor, y_train_tensor, x_valid_tensor, y_valid_tensor,device)\n",
    "\n",
    "test_output_y = tra_weightNN.predict(x_test_tensor,device)\n",
    "test_accuracy = accuracy(test_output_y, y_test_tensor.data.numpy())\n",
    "\n",
    "print('test accuracy is {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_test = []\n",
    "rewgt_test = []\n",
    "for rep in range(10):\n",
    "    lenet = LeNet()\n",
    "    lenet.to(device)\n",
    "\n",
    "    stand_trainNN = StandardTrainingNN(lenet,\n",
    "                                       batch_size=50, \n",
    "                                       num_iter=80,\n",
    "                                       learning_rate=1e-3,\n",
    "                                       early_stopping=10,\n",
    "                                       iprint=0)\n",
    "    stand_trainNN.fit(x_train_tensor, y_train_tensor, x_valid_tensor, y_valid_tensor,device)\n",
    "\n",
    "    test_output_y = stand_trainNN.predict(x_test_tensor,device)\n",
    "    test_accuracy = accuracy(test_output_y, y_test_tensor.data.numpy())\n",
    "    std_test.append(test_accuracy)\n",
    "    \n",
    "    time.sleep(10)\n",
    "    \n",
    "    lenet = LeNet()\n",
    "    lenet.to(device)\n",
    "\n",
    "    tra_weightNN = TrajectoryReweightNN(lenet,\n",
    "                                        burnin=10, \n",
    "                                        num_cluster=5,\n",
    "                                        batch_size=50, \n",
    "                                        num_iter=70, \n",
    "                                        learning_rate=1e-3,\n",
    "                                        early_stopping=10,\n",
    "                                        iprint=0)\n",
    "    tra_weightNN.fit(x_train_tensor, y_train_tensor, x_valid_tensor, y_valid_tensor,device)\n",
    "\n",
    "    test_output_y = tra_weightNN.predict(x_test_tensor,device)\n",
    "    test_accuracy = accuracy(test_output_y, y_test_tensor.data.numpy())\n",
    "    rewgt_test.append(test_accuracy)\n",
    "    \n",
    "    time.sleep(10)\n",
    "    \n",
    "    print('running {}/10'.format(rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(std_test)\n",
    "print(rewgt_test)\n",
    "print(np.mean(std_test),np.mean(rewgt_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
