{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "from trajectory_classifier.gmm import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from scipy import spatial\n",
    "from util.util import mnist_noise, correct_prob\n",
    "from util.util import ImbalancedDatasetSampler\n",
    "\n",
    "from trajectoryReweight.model import WeightedCrossEntropyLoss, TrajectoryReweightNN\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CNN & LeNet\n",
    "\"\"\"\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.drop_out = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(7 * 7 * 64, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def name(self):\n",
    "        return \"LeNet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "MNIST DATA\n",
    "\"\"\"\n",
    "dataset = np.load('data/mnist.npz')\n",
    "valid_idx = np.random.choice(range(60000), size=1000, replace=False)\n",
    "x_valid = dataset['x_train'][valid_idx]\n",
    "y_valid = dataset['y_train'][valid_idx]\n",
    "x_train = np.delete(dataset['x_train'], valid_idx, axis=0)\n",
    "y_train = np.delete(dataset['y_train'], valid_idx)\n",
    "x_test = dataset['x_test']\n",
    "y_test = dataset['y_test']\n",
    "\n",
    "\"\"\"\n",
    "shrink dataset to make noisy significant\n",
    "\"\"\"\n",
    "x_train = x_train[:10000]\n",
    "y_train = y_train[:10000]\n",
    "\n",
    "\"\"\"\n",
    "Add Noise to training data\n",
    "\"\"\"\n",
    "y_train_noisy = mnist_noise(y_train,0.6)\n",
    "\n",
    "\"\"\"\n",
    "Plot an example\n",
    "\"\"\" \n",
    "plt.imshow(x_train[0], cmap='gray')\n",
    "plt.title('%i' % y_train[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initializaion\n",
    "\"\"\"\n",
    "train_idx = np.arange(len(x_train))\n",
    "x_train = np.transpose(x_train,(2,1,0))\n",
    "x_valid = np.transpose(x_valid,(2,1,0))\n",
    "x_test = np.transpose(x_test,(2,1,0))\n",
    "x_train_tensor = torchvision.transforms.ToTensor()(x_train).unsqueeze(1)\n",
    "x_valid_tensor = torchvision.transforms.ToTensor()(x_valid).unsqueeze(1)\n",
    "x_test_tensor = torchvision.transforms.ToTensor()(x_test).unsqueeze(1)\n",
    "y_train_tensor = torch.from_numpy(y_train.astype(np.long))\n",
    "y_train_noisy_tensor = torch.from_numpy(y_train_noisy.astype(np.long))\n",
    "y_valid_tensor = torch.from_numpy(y_valid.astype(np.long))\n",
    "y_test_tensor = torch.from_numpy(y_test.astype(np.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1 | loss = 2.1346 | test accuracy = 0.6436\n",
      "epoch = 2 | loss = 2.0710 | test accuracy = 0.7986\n",
      "epoch = 3 | loss = 1.9994 | test accuracy = 0.8491\n",
      "epoch = 4 | loss = 2.0500 | test accuracy = 0.8710\n",
      "epoch = 5 | loss = 2.1704 | test accuracy = 0.8805\n",
      "epoch = 6 | loss = 1.9108 | test accuracy = 0.8896\n",
      "epoch = 7 | loss = 1.9990 | test accuracy = 0.8914\n",
      "epoch = 8 | loss = 2.1261 | test accuracy = 0.9049\n",
      "epoch = 9 | loss = 1.9972 | test accuracy = 0.9205\n",
      "epoch = 10 | loss = 1.9778 | test accuracy = 0.9154\n",
      "epoch = 11 | loss = 2.0325 | test accuracy = 0.9254\n",
      "epoch = 12 | loss = 2.0687 | test accuracy = 0.9256\n",
      "epoch = 13 | loss = 1.9650 | test accuracy = 0.9221\n",
      "epoch = 14 | loss = 2.0028 | test accuracy = 0.9208\n",
      "epoch = 15 | loss = 1.9846 | test accuracy = 0.9215\n",
      "epoch = 16 | loss = 1.9391 | test accuracy = 0.9259\n",
      "epoch = 17 | loss = 2.0435 | test accuracy = 0.9236\n",
      "epoch = 18 | loss = 1.9560 | test accuracy = 0.9225\n",
      "epoch = 19 | loss = 2.0526 | test accuracy = 0.9265\n",
      "epoch = 20 | loss = 1.9857 | test accuracy = 0.9265\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CNN without reweight\n",
    "\"\"\"\n",
    "cnn = CNN()\n",
    "cnn.to(device)\n",
    "\n",
    "num_iter = 20\n",
    "lr = 5e-5\n",
    "batch_size = 100\n",
    "L2 = 0.0005\n",
    "\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=lr, weight_decay=L2)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "tensor_noisy_dataset = Data.TensorDataset(x_train_tensor,y_train_noisy_tensor)\n",
    "train_noisy_loader= Data.DataLoader(dataset=tensor_noisy_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(num_iter):\n",
    "    for step, (b_x, b_y) in enumerate(train_noisy_loader):\n",
    "        b_x, b_y = b_x.to(device), b_y.to(device)\n",
    "        output = cnn(b_x)\n",
    "        loss = loss_func(output, b_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_output = cnn(x_test_tensor.to(device))\n",
    "        test_output_y = torch.max(test_output, 1)[1].data.cpu().numpy()\n",
    "        test_accuracy = float((test_output_y == y_test_tensor.data.numpy()).astype(int).sum()) / float(y_test_tensor.size(0))\n",
    "        print('epoch = {} | loss = {:.4f} | test accuracy = {:.4f}'.format(epoch+1, loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 5 burn-in epoch...\n",
      "Train 5 burn-in epoch complete.\n",
      "------------------------------------------------------------\n",
      "Trajectory clustering for burn-in epoch...\n",
      "Trajectory clustering for burn-in epoch complete.\n",
      "------------------------------------------------------------\n",
      "Trajectory based training start ...\n",
      "\n",
      "|-------------------------------------------------------------------\n",
      "| - epoch = 2\n",
      "| - compute valid set grad...\n",
      "| - update cluster weight based on valid set...\n",
      "| -  {0: 0, 'size': 1049, 'weight': 1.0855848789215088}\n",
      "| -  {1: 1, 'size': 1089, 'weight': 1.0906578302383423}\n",
      "| -  {2: 2, 'size': 854, 'weight': 1.0738710165023804}\n",
      "| -  {3: 3, 'size': 1511, 'weight': 0.9213136434555054}\n",
      "| -  {4: 4, 'size': 1092, 'weight': 0.9590571522712708}\n",
      "| -  {5: 5, 'size': 579, 'weight': 1.0819971561431885}\n",
      "| -  {6: 6, 'size': 757, 'weight': 1.0273687839508057}\n",
      "| -  {7: 7, 'size': 1190, 'weight': 0.9128358364105225}\n",
      "| -  {8: 8, 'size': 1219, 'weight': 1.0930501222610474}\n",
      "| -  {9: 9, 'size': 660, 'weight': 0.9082955718040466}\n",
      "| - mini-batch training based on cluster weights...\n",
      "| - epoch = 2 | loss = 1.8279 | valid error = 0.1040\n",
      "| - update trajectory cluster...\n",
      "|-------------------------------------------------------------------\n",
      "\n",
      "|-------------------------------------------------------------------\n",
      "| - epoch = 3\n",
      "| - compute valid set grad...\n",
      "| - update cluster weight based on valid set...\n",
      "| -  {0: 0, 'size': 1049, 'weight': 1.1706444025039673}\n",
      "| -  {1: 1, 'size': 1089, 'weight': 1.1802035570144653}\n",
      "| -  {2: 2, 'size': 854, 'weight': 1.1446930170059204}\n",
      "| -  {3: 3, 'size': 1511, 'weight': 0.8343951106071472}\n",
      "| -  {4: 4, 'size': 1092, 'weight': 0.9031094908714294}\n",
      "| -  {5: 5, 'size': 579, 'weight': 1.1616486310958862}\n",
      "| -  {6: 6, 'size': 757, 'weight': 1.040177822113037}\n",
      "| -  {7: 7, 'size': 1190, 'weight': 0.8263050317764282}\n",
      "| -  {8: 8, 'size': 1219, 'weight': 1.185775637626648}\n",
      "| -  {9: 9, 'size': 660, 'weight': 0.8214824199676514}\n",
      "| - mini-batch training based on cluster weights...\n",
      "| - epoch = 3 | loss = 1.8669 | valid error = 0.1010\n",
      "| - update trajectory cluster...\n",
      "|-------------------------------------------------------------------\n",
      "\n",
      "|-------------------------------------------------------------------\n",
      "| - epoch = 4\n",
      "| - compute valid set grad...\n",
      "| - update cluster weight based on valid set...\n",
      "| -  {0: 0, 'size': 1049, 'weight': 1.2538211345672607}\n",
      "| -  {1: 1, 'size': 1089, 'weight': 1.2701396942138672}\n",
      "| -  {2: 2, 'size': 854, 'weight': 1.2170096635818481}\n",
      "| -  {3: 3, 'size': 1511, 'weight': 0.7516756057739258}\n",
      "| -  {4: 4, 'size': 1092, 'weight': 0.8439157605171204}\n",
      "| -  {5: 5, 'size': 579, 'weight': 1.2408227920532227}\n",
      "| -  {6: 6, 'size': 757, 'weight': 1.044922947883606}\n",
      "| -  {7: 7, 'size': 1190, 'weight': 0.7405326962471008}\n",
      "| -  {8: 8, 'size': 1219, 'weight': 1.278109073638916}\n",
      "| -  {9: 9, 'size': 660, 'weight': 0.7346243262290955}\n",
      "| - mini-batch training based on cluster weights...\n",
      "| - epoch = 4 | loss = 1.6179 | valid error = 0.0900\n",
      "| - update trajectory cluster...\n",
      "|-------------------------------------------------------------------\n",
      "\n",
      "|-------------------------------------------------------------------\n",
      "| - epoch = 5\n",
      "| - compute valid set grad...\n",
      "| - update cluster weight based on valid set...\n",
      "| -  {0: 0, 'size': 916, 'weight': 0.6489325165748596}\n",
      "| -  {1: 1, 'size': 638, 'weight': 1.0521841049194336}\n",
      "| -  {2: 2, 'size': 1430, 'weight': 0.6642365455627441}\n",
      "| -  {3: 3, 'size': 593, 'weight': 1.32866370677948}\n",
      "| -  {4: 4, 'size': 1209, 'weight': 1.371650218963623}\n",
      "| -  {5: 5, 'size': 1125, 'weight': 1.33792245388031}\n",
      "| -  {6: 6, 'size': 804, 'weight': 1.2931233644485474}\n",
      "| -  {7: 7, 'size': 1410, 'weight': 0.648568868637085}\n",
      "| -  {8: 8, 'size': 849, 'weight': 0.6752114295959473}\n",
      "| -  {9: 9, 'size': 1026, 'weight': 1.3095769882202148}\n",
      "| - mini-batch training based on cluster weights...\n",
      "| - epoch = 5 | loss = 1.5121 | valid error = 0.0820\n",
      "| - update trajectory cluster...\n",
      "|-------------------------------------------------------------------\n",
      "\n",
      "|-------------------------------------------------------------------\n",
      "| - epoch = 6\n",
      "| - compute valid set grad...\n",
      "| - update cluster weight based on valid set...\n",
      "| -  {0: 0, 'size': 916, 'weight': 0.5562124848365784}\n",
      "| -  {1: 1, 'size': 638, 'weight': 1.0607216358184814}\n",
      "| -  {2: 2, 'size': 1430, 'weight': 0.5828108787536621}\n",
      "| -  {3: 3, 'size': 593, 'weight': 1.4160268306732178}\n",
      "| -  {4: 4, 'size': 1209, 'weight': 1.4647884368896484}\n",
      "| -  {5: 5, 'size': 1125, 'weight': 1.4209859371185303}\n",
      "| -  {6: 6, 'size': 804, 'weight': 1.3714367151260376}\n",
      "| -  {7: 7, 'size': 1410, 'weight': 0.5584781169891357}\n",
      "| -  {8: 8, 'size': 849, 'weight': 0.6039655208587646}\n",
      "| -  {9: 9, 'size': 1026, 'weight': 1.403684377670288}\n",
      "| - mini-batch training based on cluster weights...\n",
      "| - epoch = 6 | loss = 1.4403 | valid error = 0.0800\n",
      "| - update trajectory cluster...\n",
      "|-------------------------------------------------------------------\n",
      "\n",
      "|-------------------------------------------------------------------\n",
      "| - epoch = 7\n",
      "| - compute valid set grad...\n",
      "| - update cluster weight based on valid set...\n",
      "| -  {0: 0, 'size': 916, 'weight': 0.46712130308151245}\n",
      "| -  {1: 1, 'size': 638, 'weight': 1.050855040550232}\n",
      "| -  {2: 2, 'size': 1430, 'weight': 0.4941847324371338}\n",
      "| -  {3: 3, 'size': 593, 'weight': 1.5007123947143555}\n",
      "| -  {4: 4, 'size': 1209, 'weight': 1.5587890148162842}\n",
      "| -  {5: 5, 'size': 1125, 'weight': 1.504631757736206}\n",
      "| -  {6: 6, 'size': 804, 'weight': 1.4480690956115723}\n",
      "| -  {7: 7, 'size': 1410, 'weight': 0.46515634655952454}\n",
      "| -  {8: 8, 'size': 849, 'weight': 0.5305091142654419}\n",
      "| -  {9: 9, 'size': 1026, 'weight': 1.4964392185211182}\n",
      "| - mini-batch training based on cluster weights...\n",
      "| - epoch = 7 | loss = 1.4487 | valid error = 0.0810\n",
      "| - update trajectory cluster...\n",
      "|-------------------------------------------------------------------\n",
      "\n",
      "|-------------------------------------------------------------------\n",
      "| - epoch = 8\n",
      "| - compute valid set grad...\n",
      "| - update cluster weight based on valid set...\n",
      "| -  {0: 0, 'size': 705, 'weight': 1.530320405960083}\n",
      "| -  {1: 1, 'size': 913, 'weight': 0.463794469833374}\n",
      "| -  {2: 2, 'size': 1234, 'weight': 1.6517539024353027}\n",
      "| -  {3: 3, 'size': 702, 'weight': 1.5845228433609009}\n",
      "| -  {4: 4, 'size': 655, 'weight': 1.0315124988555908}\n",
      "| -  {5: 5, 'size': 734, 'weight': 0.37966352701187134}\n",
      "| -  {6: 6, 'size': 934, 'weight': 1.6427860260009766}\n",
      "| -  {7: 7, 'size': 1384, 'weight': 0.38806524872779846}\n",
      "| -  {8: 8, 'size': 1268, 'weight': 1.6387877464294434}\n",
      "| -  {9: 9, 'size': 1471, 'weight': 0.42423638701438904}\n",
      "| - mini-batch training based on cluster weights...\n",
      "| - epoch = 8 | loss = 1.2581 | valid error = 0.0780\n",
      "| - update trajectory cluster...\n",
      "|-------------------------------------------------------------------\n",
      "\n",
      "|-------------------------------------------------------------------\n",
      "| - epoch = 9\n",
      "| - compute valid set grad...\n",
      "| - update cluster weight based on valid set...\n",
      "| -  {0: 0, 'size': 705, 'weight': 1.6089595556259155}\n",
      "| -  {1: 1, 'size': 913, 'weight': 0.3842419385910034}\n",
      "| -  {2: 2, 'size': 1234, 'weight': 1.743719458580017}\n",
      "| -  {3: 3, 'size': 702, 'weight': 1.666555404663086}\n",
      "| -  {4: 4, 'size': 655, 'weight': 0.9913145899772644}\n",
      "| -  {5: 5, 'size': 734, 'weight': 0.28834688663482666}\n",
      "| -  {6: 6, 'size': 934, 'weight': 1.7358503341674805}\n",
      "| -  {7: 7, 'size': 1384, 'weight': 0.30399104952812195}\n",
      "| -  {8: 8, 'size': 1268, 'weight': 1.7169817686080933}\n",
      "| -  {9: 9, 'size': 1471, 'weight': 0.3444433808326721}\n",
      "| - mini-batch training based on cluster weights...\n",
      "| - epoch = 9 | loss = 1.0510 | valid error = 0.0650\n",
      "| - update trajectory cluster...\n",
      "|-------------------------------------------------------------------\n",
      "\n",
      "|-------------------------------------------------------------------\n",
      "| - epoch = 10\n",
      "| - compute valid set grad...\n",
      "| - update cluster weight based on valid set...\n",
      "| -  {0: 0, 'size': 705, 'weight': 1.6840132474899292}\n",
      "| -  {1: 1, 'size': 913, 'weight': 0.29937613010406494}\n",
      "| -  {2: 2, 'size': 1234, 'weight': 1.834956407546997}\n",
      "| -  {3: 3, 'size': 702, 'weight': 1.742822289466858}\n",
      "| -  {4: 4, 'size': 655, 'weight': 0.9310451745986938}\n",
      "| -  {5: 5, 'size': 734, 'weight': 0.19988569617271423}\n",
      "| -  {6: 6, 'size': 934, 'weight': 1.827205777168274}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| -  {7: 7, 'size': 1384, 'weight': 0.2161998152732849}\n",
      "| -  {8: 8, 'size': 1268, 'weight': 1.7933977842330933}\n",
      "| -  {9: 9, 'size': 1471, 'weight': 0.25964587926864624}\n",
      "| - mini-batch training based on cluster weights...\n",
      "| - epoch = 10 | loss = 0.8367 | valid error = 0.0770\n",
      "| - update trajectory cluster...\n",
      "|-------------------------------------------------------------------\n",
      "\n",
      "|-------------------------------------------------------------------\n",
      "| - epoch = 11\n",
      "| - compute valid set grad...\n",
      "| - update cluster weight based on valid set...\n",
      "| -  {0: 0, 'size': 984, 'weight': 1.9205034971237183}\n",
      "| -  {1: 1, 'size': 1236, 'weight': 1.870374321937561}\n",
      "| -  {2: 2, 'size': 832, 'weight': 0.11573831737041473}\n",
      "| -  {3: 3, 'size': 1449, 'weight': 0.1374794989824295}\n",
      "| -  {4: 4, 'size': 649, 'weight': 0.8934488296508789}\n",
      "| -  {5: 5, 'size': 730, 'weight': 1.768159031867981}\n",
      "| -  {6: 6, 'size': 856, 'weight': 0.21453356742858887}\n",
      "| -  {7: 7, 'size': 1206, 'weight': 1.923339605331421}\n",
      "| -  {8: 8, 'size': 1410, 'weight': 0.18037471175193787}\n",
      "| -  {9: 9, 'size': 648, 'weight': 1.8244545459747314}\n",
      "| - mini-batch training based on cluster weights...\n",
      "| - epoch = 11 | loss = 0.7192 | valid error = 0.0760\n",
      "| - update trajectory cluster...\n",
      "|-------------------------------------------------------------------\n",
      "\n",
      "|-------------------------------------------------------------------\n",
      "| - epoch = 12\n",
      "| - compute valid set grad...\n",
      "| - update cluster weight based on valid set...\n",
      "| -  {0: 0, 'size': 984, 'weight': 2.0093066692352295}\n",
      "| -  {1: 1, 'size': 1236, 'weight': 1.9338010549545288}\n",
      "| -  {2: 2, 'size': 832, 'weight': 0.03145687282085419}\n",
      "| -  {3: 3, 'size': 1449, 'weight': 0.05636972188949585}\n",
      "| -  {4: 4, 'size': 649, 'weight': 0.8373919129371643}\n",
      "| -  {5: 5, 'size': 730, 'weight': 1.8493074178695679}\n",
      "| -  {6: 6, 'size': 856, 'weight': 0.12786059081554413}\n",
      "| -  {7: 7, 'size': 1206, 'weight': 2.0037050247192383}\n",
      "| -  {8: 8, 'size': 1410, 'weight': 0.09936537593603134}\n",
      "| -  {9: 9, 'size': 648, 'weight': 1.8949129581451416}\n",
      "| - mini-batch training based on cluster weights...\n",
      "| - epoch = 12 | loss = 0.5982 | valid error = 0.0620\n",
      "| - update trajectory cluster...\n",
      "|-------------------------------------------------------------------\n",
      "\n",
      "|-------------------------------------------------------------------\n",
      "| - epoch = 13\n",
      "| - compute valid set grad...\n",
      "| - update cluster weight based on valid set...\n",
      "| -  {0: 0, 'size': 984, 'weight': 2.087404727935791}\n",
      "| -  {1: 1, 'size': 1236, 'weight': 1.9621163606643677}\n",
      "| -  {2: 2, 'size': 832, 'weight': 0.009999999776482582}\n",
      "| -  {3: 3, 'size': 1449, 'weight': 0.010296761989593506}\n",
      "| -  {4: 4, 'size': 649, 'weight': 0.8045249581336975}\n",
      "| -  {5: 5, 'size': 730, 'weight': 1.9260199069976807}\n",
      "| -  {6: 6, 'size': 856, 'weight': 0.07199463248252869}\n",
      "| -  {7: 7, 'size': 1206, 'weight': 2.066310167312622}\n",
      "| -  {8: 8, 'size': 1410, 'weight': 0.059299591928720474}\n",
      "| -  {9: 9, 'size': 648, 'weight': 1.9354734420776367}\n",
      "| - mini-batch training based on cluster weights...\n",
      "| - epoch = 13 | loss = 0.4276 | valid error = 0.0670\n",
      "| - update trajectory cluster...\n",
      "|-------------------------------------------------------------------\n",
      "\n",
      "|-------------------------------------------------------------------\n",
      "| - epoch = 14\n",
      "| - compute valid set grad...\n",
      "| - update cluster weight based on valid set...\n",
      "| -  {0: 0, 'size': 680, 'weight': 1.9931169748306274}\n",
      "| -  {1: 1, 'size': 609, 'weight': 0.04371274635195732}\n",
      "| -  {2: 2, 'size': 1319, 'weight': 0.009999999776482582}\n",
      "| -  {3: 3, 'size': 1266, 'weight': 2.1320414543151855}\n",
      "| -  {4: 4, 'size': 1403, 'weight': 0.03216218948364258}\n",
      "| -  {5: 5, 'size': 1288, 'weight': 2.0014846324920654}\n",
      "| -  {6: 6, 'size': 892, 'weight': 0.15172144770622253}\n",
      "| -  {7: 7, 'size': 1005, 'weight': 2.170559883117676}\n",
      "| -  {8: 8, 'size': 690, 'weight': 1.9886139631271362}\n",
      "| -  {9: 9, 'size': 848, 'weight': 0.009999999776482582}\n",
      "| - mini-batch training based on cluster weights...\n",
      "| - epoch = 14 | loss = 0.5365 | valid error = 0.0640\n",
      "| - update trajectory cluster...\n",
      "|-------------------------------------------------------------------\n",
      "\n",
      "|-------------------------------------------------------------------\n",
      "| - epoch = 15\n",
      "| - compute valid set grad...\n",
      "| - update cluster weight based on valid set...\n",
      "| -  {0: 0, 'size': 680, 'weight': 2.038663148880005}\n",
      "| -  {1: 1, 'size': 609, 'weight': 0.011179465800523758}\n",
      "| -  {2: 2, 'size': 1319, 'weight': 0.009999999776482582}\n",
      "| -  {3: 3, 'size': 1266, 'weight': 2.1864054203033447}\n",
      "| -  {4: 4, 'size': 1403, 'weight': 0.009999999776482582}\n",
      "| -  {5: 5, 'size': 1288, 'weight': 2.042633295059204}\n",
      "| -  {6: 6, 'size': 892, 'weight': 0.10763780772686005}\n",
      "| -  {7: 7, 'size': 1005, 'weight': 2.247405529022217}\n",
      "| -  {8: 8, 'size': 690, 'weight': 2.0411972999572754}\n",
      "| -  {9: 9, 'size': 848, 'weight': 0.009999999776482582}\n",
      "| - mini-batch training based on cluster weights...\n",
      "| - epoch = 15 | loss = 0.4003 | valid error = 0.0690\n",
      "| - update trajectory cluster...\n",
      "|-------------------------------------------------------------------\n",
      "\n",
      "|-------------------------------------------------------------------\n",
      "| - epoch = 16\n",
      "| - compute valid set grad...\n",
      "| - update cluster weight based on valid set...\n",
      "| -  {0: 0, 'size': 680, 'weight': 2.085874080657959}\n",
      "| -  {1: 1, 'size': 609, 'weight': 0.009999999776482582}\n",
      "| -  {2: 2, 'size': 1319, 'weight': 0.009999999776482582}\n",
      "| -  {3: 3, 'size': 1266, 'weight': 2.251413583755493}\n",
      "| -  {4: 4, 'size': 1403, 'weight': 0.009999999776482582}\n",
      "| -  {5: 5, 'size': 1288, 'weight': 2.092191219329834}\n",
      "| -  {6: 6, 'size': 892, 'weight': 0.07295896112918854}\n",
      "| -  {7: 7, 'size': 1005, 'weight': 2.327214479446411}\n",
      "| -  {8: 8, 'size': 690, 'weight': 2.097546339035034}\n",
      "| -  {9: 9, 'size': 848, 'weight': 0.009999999776482582}\n",
      "| - mini-batch training based on cluster weights...\n",
      "| - epoch = 16 | loss = 0.2940 | valid error = 0.0610\n",
      "| - update trajectory cluster...\n",
      "|-------------------------------------------------------------------\n",
      "\n",
      "Trajectory based training complete, best validation error = 0.061000000000000054 at epoch = 20.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CNN with reweight\n",
    "\"\"\"\n",
    "\n",
    "cnn = CNN()\n",
    "cnn.to(device)\n",
    "\n",
    "tra_weightNN = TrajectoryReweightNN(cnn, \n",
    "                                    burnin=5, \n",
    "                                    num_cluster=10, \n",
    "                                    batch_size=100, \n",
    "                                    num_iter=15, \n",
    "                                    learning_rate=5e-5, \n",
    "                                    early_stopping=10)\n",
    "tra_weightNN.fit(x_train_tensor, y_train_noisy_tensor, x_valid_tensor, y_valid_tensor,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy is 0.9459\n"
     ]
    }
   ],
   "source": [
    "test_output_y = tra_weightNN.predict(x_test_tensor,device)\n",
    "test_accuracy = float((test_output_y == y_test_tensor.data.numpy()).astype(int).sum()) / float(y_test_tensor.size(0))\n",
    "print('test accuracy is {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imbalanced MNIST DATA\n",
    "\"\"\"\n",
    "dataset = np.load('data/mnist.npz')\n",
    "\n",
    "four_index = dataset['y_train'] == 4\n",
    "nine_index = dataset['y_train'] == 9\n",
    "\n",
    "y_fours = dataset['y_train'][four_index]-4\n",
    "y_nines = dataset['y_train'][nine_index]-8\n",
    "x_fours = dataset['x_train'][four_index]\n",
    "x_nines = dataset['x_train'][nine_index]\n",
    "\n",
    "valid_four_index = np.random.choice(range(len(y_fours)), size=250, replace=False)\n",
    "valid_nine_index = np.random.choice(range(len(y_nines)), size=250, replace=False)\n",
    "\n",
    "x_valid_four =  x_fours[valid_four_index]\n",
    "x_valid_nine =  x_nines[valid_nine_index]\n",
    "y_valid_four =  y_fours[valid_four_index]\n",
    "y_valid_nine =  y_nines[valid_nine_index]\n",
    "\n",
    "x_valid = np.append(x_valid_four,x_valid_nine,0)\n",
    "y_valid = np.append(y_valid_four,y_valid_nine)\n",
    "indices = np.arange(x_valid.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "x_valid = x_valid[indices]\n",
    "y_valid = y_valid[indices]\n",
    "\n",
    "x_train_four = np.delete(x_fours, valid_four_index, axis=0)\n",
    "y_train_four = np.delete(y_fours, valid_four_index)\n",
    "x_train_nine = np.delete(x_nines, valid_four_index, axis=0)\n",
    "y_train_nine = np.delete(y_nines, valid_four_index)\n",
    "\n",
    "four_index = dataset['y_test'] == 4\n",
    "nine_index = dataset['y_test'] == 9\n",
    "\n",
    "y_fours = dataset['y_test'][four_index]-4\n",
    "y_nines = dataset['y_test'][nine_index]-8\n",
    "x_fours = dataset['x_test'][four_index]\n",
    "x_nines = dataset['x_test'][nine_index]\n",
    "x_test = np.append(x_fours,x_nines,0)\n",
    "y_test = np.append(y_fours,y_nines)\n",
    "indices = np.arange(x_test.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "x_test = x_test[indices]\n",
    "y_test = y_test[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio: 4:9 = 31:4969\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Imbalanced MNIST Study\n",
    "\"\"\" \n",
    "a = [500,250,125,62,31]\n",
    "\n",
    "for ratio in [31]:\n",
    "    print('ratio: 4:9 = {}:{}'.format(ratio, 5000-ratio))\n",
    "    four_part = np.random.choice(range(len(x_train_four)), size=ratio, replace=False)\n",
    "    nine_part = np.random.choice(range(len(x_train_nine)), size=5000-ratio, replace=False)\n",
    "    x_train = np.append(x_train_four[four_part],x_train_nine[nine_part],0)\n",
    "    y_train = np.append(y_train_four[four_part],y_train_nine[nine_part])\n",
    "    indices = np.arange(x_train.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    x_train = x_train[indices]\n",
    "    y_train = y_train[indices]\n",
    "\n",
    "\n",
    "    x_train = np.transpose(x_train,(2,1,0))\n",
    "    x_valid = np.transpose(x_valid,(2,1,0))\n",
    "    x_test = np.transpose(x_test,(2,1,0))\n",
    "    x_train_tensor = torchvision.transforms.ToTensor()(x_train).unsqueeze(1)\n",
    "    x_valid_tensor = torchvision.transforms.ToTensor()(x_valid).unsqueeze(1)\n",
    "    x_test_tensor = torchvision.transforms.ToTensor()(x_test).unsqueeze(1)\n",
    "    y_train_tensor = torch.from_numpy(y_train.astype(np.long))\n",
    "    y_valid_tensor = torch.from_numpy(y_valid.astype(np.long))\n",
    "    y_test_tensor = torch.from_numpy(y_test.astype(np.long))\n",
    "\n",
    "\n",
    "#     \"\"\"\n",
    "#     baseline\n",
    "#     \"\"\"\n",
    "\n",
    "#     lenet = LeNet().to(device=device)\n",
    "#     num_iter = 80\n",
    "#     lr = 0.001\n",
    "#     batch_size = 100\n",
    "#     L2 = 0.0005\n",
    "\n",
    "\n",
    "#     tensor_dataset = Data.TensorDataset(x_train_tensor,y_train_tensor)\n",
    "#     train_loader= Data.DataLoader(dataset=tensor_dataset, batch_size=batch_size, shuffle=True)\n",
    "#     optimizer = torch.optim.Adam(lenet.parameters(), lr=lr, weight_decay=L2)\n",
    "#     loss_func = nn.CrossEntropyLoss()\n",
    "#     for epoch in range(num_iter):\n",
    "#         for step, (b_x, b_y) in enumerate(train_loader):\n",
    "#             b_x, b_y = b_x.to(device), b_y.to(device, torch.long)\n",
    "#             output = lenet(b_x)\n",
    "#             loss = loss_func(output, b_y)\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         test_output = lenet(x_test_tensor.to(device))\n",
    "#         test_output_y = torch.max(test_output, 1)[1].data.cpu().numpy()\n",
    "#         acc = 0\n",
    "#         for pred, y in zip(test_output_y,y_test_tensor.data.numpy()):\n",
    "#             if pred == y:\n",
    "#                 acc += 1\n",
    "#         test_accuracy = acc / float(y_test_tensor.size(0))\n",
    "#         print('lenet baseline test error is ', 1-test_accuracy)\n",
    "\n",
    "#     \"\"\"\n",
    "#     adjust class weight\n",
    "#     \"\"\"\n",
    "#     lenet = LeNet().to(device=device)\n",
    "#     optimizer = torch.optim.Adam(lenet.parameters(), lr=lr, weight_decay=L2)\n",
    "#     weight = [float(5000/ratio), float(5000/(5000-ratio))]\n",
    "#     weight = torch.FloatTensor(weight).to(device)\n",
    "#     loss_func = nn.CrossEntropyLoss(weight=weight)\n",
    "#     for epoch in range(num_iter):\n",
    "#         for step, (b_x, b_y) in enumerate(train_loader):\n",
    "#             b_x, b_y = b_x.to(device), b_y.to(device, torch.long)\n",
    "#             output = lenet(b_x)\n",
    "#             loss = loss_func(output, b_y)\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         test_output = lenet(x_test_tensor.to(device))\n",
    "#         test_output_y = torch.max(test_output, 1)[1].data.cpu().numpy()\n",
    "#         acc = 0\n",
    "#         for pred, y in zip(test_output_y,y_test_tensor.data.numpy()):\n",
    "#             if pred == y:\n",
    "#                 acc += 1\n",
    "#         test_accuracy = acc / float(y_test_tensor.size(0))\n",
    "#         print('lenet with class weight test error is ', 1-test_accuracy)\n",
    "\n",
    "#     \"\"\"\n",
    "#     resample to balance\n",
    "#     \"\"\"\n",
    "#     lenet = LeNet().to(device=device)\n",
    "#     optimizer = torch.optim.Adam(lenet.parameters(), lr=lr, weight_decay=L2)\n",
    "#     tensor_dataset = Data.TensorDataset(x_train_tensor,y_train_tensor)\n",
    "#     train_loader= Data.DataLoader(dataset=tensor_dataset, batch_size=batch_size, sampler=ImbalancedDatasetSampler(tensor_dataset), shuffle=False)\n",
    "#     loss_func = nn.CrossEntropyLoss()\n",
    "#     for epoch in range(num_iter):\n",
    "#         for step, (b_x, b_y) in enumerate(train_loader):\n",
    "#             b_x, b_y = b_x.to(device), b_y.to(device, torch.long)\n",
    "#             output = lenet(b_x)\n",
    "#             loss = loss_func(output, b_y)\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         test_output = lenet(x_test_tensor.to(device))\n",
    "#         test_output_y = torch.max(test_output, 1)[1].data.cpu().numpy()\n",
    "#         acc = 0\n",
    "#         for pred, y in zip(test_output_y,y_test_tensor.data.numpy()):\n",
    "#             if pred == y:\n",
    "#                 acc += 1\n",
    "#         test_accuracy = acc / float(y_test_tensor.size(0))\n",
    "#         print('lenet with imbalance sampling test error is ', 1-test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 10 burn-in epoch...\n",
      "Train 10 burn-in epoch complete.\n",
      "------------------------------------------------------------\n",
      "Trajectory clustering for burn-in epoch...\n",
      "Trajectory clustering for burn-in epoch complete.\n",
      "------------------------------------------------------------\n",
      "Trajectory based training start ...\n",
      "\n",
      "|-------------------------------------------------------------------\n",
      "| - epoch = 2\n",
      "| - compute valid set grad...\n",
      "| - update cluster weight based on valid set...\n",
      "| -  {4: 4, 'size': 5000, 'weight': 1.0663399696350098}\n",
      "| - mini-batch training based on cluster weights...\n",
      "| - epoch = 2 | loss = 0.0015 | valid error = 0.0560\n",
      "| - update trajectory cluster...\n",
      "|-------------------------------------------------------------------\n",
      "\n",
      "|-------------------------------------------------------------------\n",
      "| - epoch = 3\n",
      "| - compute valid set grad...\n",
      "| - update cluster weight based on valid set...\n",
      "| -  {4: 4, 'size': 5000, 'weight': 0.97083580493927}\n",
      "| - mini-batch training based on cluster weights...\n",
      "| - epoch = 3 | loss = 0.0000 | valid error = 0.1400\n",
      "| - update trajectory cluster...\n",
      "|-------------------------------------------------------------------\n",
      "\n",
      "|-------------------------------------------------------------------\n",
      "| - epoch = 4\n",
      "| - compute valid set grad...\n",
      "| - update cluster weight based on valid set...\n",
      "| -  {4: 4, 'size': 5000, 'weight': 1.049742341041565}\n",
      "| - mini-batch training based on cluster weights...\n",
      "| - epoch = 4 | loss = 0.0002 | valid error = 0.0720\n",
      "| - update trajectory cluster...\n",
      "|-------------------------------------------------------------------\n",
      "\n",
      "|-------------------------------------------------------------------\n",
      "| - epoch = 5\n",
      "| - compute valid set grad...\n",
      "| - update cluster weight based on valid set...\n",
      "| -  {1: 1, 'size': 5000, 'weight': 0.953996479511261}\n",
      "| - mini-batch training based on cluster weights...\n",
      "| - epoch = 5 | loss = 0.0001 | valid error = 0.1240\n",
      "| - update trajectory cluster...\n",
      "|-------------------------------------------------------------------\n",
      "\n",
      "|-------------------------------------------------------------------\n",
      "| - epoch = 6\n",
      "| - compute valid set grad...\n",
      "| - update cluster weight based on valid set...\n",
      "| -  {1: 1, 'size': 5000, 'weight': 1.029894232749939}\n",
      "| - mini-batch training based on cluster weights...\n",
      "| - epoch = 6 | loss = 0.0040 | valid error = 0.1240\n",
      "| - update trajectory cluster...\n",
      "|-------------------------------------------------------------------\n",
      "\n",
      "|-------------------------------------------------------------------\n",
      "| - epoch = 7\n",
      "| - compute valid set grad...\n",
      "| - update cluster weight based on valid set...\n",
      "| -  {1: 1, 'size': 5000, 'weight': 1.116105556488037}\n",
      "| - mini-batch training based on cluster weights...\n",
      "| - epoch = 7 | loss = 0.0013 | valid error = 0.0960\n",
      "| - update trajectory cluster...\n",
      "|-------------------------------------------------------------------\n",
      "\n",
      "|-------------------------------------------------------------------\n",
      "| - epoch = 8\n",
      "| - compute valid set grad...\n",
      "| - update cluster weight based on valid set...\n",
      "| -  {0: 0, 'size': 5000, 'weight': 1.0888582468032837}\n",
      "| - mini-batch training based on cluster weights...\n",
      "| - epoch = 8 | loss = 0.0018 | valid error = 0.0600\n",
      "| - update trajectory cluster...\n",
      "|-------------------------------------------------------------------\n",
      "\n",
      "|-------------------------------------------------------------------\n",
      "| - epoch = 9\n",
      "| - compute valid set grad...\n",
      "| - update cluster weight based on valid set...\n",
      "| -  {0: 0, 'size': 5000, 'weight': 1.0054106712341309}\n",
      "| - mini-batch training based on cluster weights...\n",
      "| - epoch = 9 | loss = 0.0007 | valid error = 0.1060\n",
      "| - update trajectory cluster...\n",
      "|-------------------------------------------------------------------\n",
      "\n",
      "|-------------------------------------------------------------------\n",
      "| - epoch = 10\n",
      "| - compute valid set grad...\n",
      "| - update cluster weight based on valid set...\n",
      "| -  {0: 0, 'size': 5000, 'weight': 1.0843439102172852}\n",
      "| - mini-batch training based on cluster weights...\n",
      "| - epoch = 10 | loss = 0.0002 | valid error = 0.0600\n",
      "| - update trajectory cluster...\n",
      "|-------------------------------------------------------------------\n",
      "\n",
      "|-------------------------------------------------------------------\n",
      "| - epoch = 11\n",
      "| - compute valid set grad...\n",
      "| - update cluster weight based on valid set...\n",
      "| -  {0: 0, 'size': 5000, 'weight': 0.9898451566696167}\n",
      "| - mini-batch training based on cluster weights...\n",
      "| - epoch = 11 | loss = 0.0000 | valid error = 0.1100\n",
      "| - update trajectory cluster...\n",
      "|-------------------------------------------------------------------\n",
      "\n",
      "|-------------------------------------------------------------------\n",
      "| - epoch = 12\n",
      "| - compute valid set grad...\n",
      "| - update cluster weight based on valid set...\n",
      "| -  {0: 0, 'size': 5000, 'weight': 1.0788851976394653}\n",
      "| - mini-batch training based on cluster weights...\n",
      "| - epoch = 12 | loss = 0.0001 | valid error = 0.1040\n",
      "| - update trajectory cluster...\n",
      "|-------------------------------------------------------------------\n",
      "\n",
      "Trajectory based training complete, best validation error = 0.05600000000000005 at epoch = 11.\n"
     ]
    }
   ],
   "source": [
    "lenet = LeNet()\n",
    "lenet.to(device)\n",
    "\n",
    "tra_weightNN = TrajectoryReweightNN(lenet, \n",
    "                                    burnin=10, \n",
    "                                    num_cluster=5, \n",
    "                                    batch_size=100, \n",
    "                                    num_iter=70, \n",
    "                                    learning_rate=1e-3,\n",
    "                                    early_stopping=10)\n",
    "tra_weightNN.fit(x_train_tensor, y_train_tensor, x_valid_tensor, y_valid_tensor,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy is 0.9422400803616273\n"
     ]
    }
   ],
   "source": [
    "test_output_y = tra_weightNN.predict(x_test_tensor,device)\n",
    "test_accuracy = float((test_output_y == y_test_tensor.data.numpy()).astype(int).sum()) / float(y_test_tensor.size(0))\n",
    "print('test accuracy is {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
